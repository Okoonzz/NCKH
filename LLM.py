# import os
# from langchain_community.vectorstores import Chroma
# from langchain_community.embeddings import SentenceTransformerEmbeddings
# from langchain_openai import ChatOpenAI
# from langchain.chains import RetrievalQA

# # Thi·∫øt l·∫≠p OpenAI key
# os.environ["OPENAI_API_KEY"] = "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"  # <-- THAY b·∫±ng key c·ªßa b·∫°n

# # Load vector store
# embedding = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
# vectordb = Chroma(persist_directory="vectordb/malware", embedding_function=embedding)

# # Load LLM t·ª´ OpenAI
# llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

# # T·∫°o pipeline RAG
# qa = RetrievalQA.from_chain_type(
#     llm=llm,
#     retriever=vectordb.as_retriever(search_kwargs={"k": 3}),
#     return_source_documents=True
# )

# # Prompt m·∫´u
# query = input(" Nh·∫≠p c√¢u h·ªèi v·ªÅ malware: ")
# result = qa(query)

# # Hi·ªÉn th·ªã k·∫øt qu·∫£
# print("\n K·∫øt qu·∫£ tr·∫£ v·ªÅ:\n")
# print(result['result'])

# print("\n Ngu·ªìn tham chi·∫øu:")
# for i, doc in enumerate(result['source_documents']):
#     print(f"- ƒêo·∫°n {i+1}:\n{doc.page_content}\n")





import os
import json
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA

# Thi·∫øt l·∫≠p OpenAI Key
os.environ["OPENAI_API_KEY"] = "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"

# # Load embedding model
# embedding = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

# # Load c√°c vector DB
# malware_tech_db = Chroma(persist_directory="vectordb2/malware2", embedding_function=embedding)
# tech_to_malware_db = Chroma(persist_directory="vectordb3/malware3", embedding_function=embedding)
# # malware_use_db = Chroma(persist_directory="vectordb4/malware4", embedding_function=embedding)
# # malware_use_pdf = Chroma(persist_directory="vectordbpdf", embedding_function=embedding)
# # malware_use_all = Chroma(persist_directory="vectordb_all", embedding_function=embedding)

# # T·∫°o retrievers
# malware_tech_retriever = malware_tech_db.as_retriever(search_kwargs={"k": 10})
# tech_to_malware_retriever = tech_to_malware_db.as_retriever(search_kwargs={"k": 20})
# # malware_use_retriever = malware_use_db.as_retriever(search_kwargs={"k": 10})
# # malware_use_retriever_pdf = malware_use_pdf.as_retriever(search_kwargs={"k": 10})
# # malware_use_retriever_all = malware_use_all.as_retriever(search_kwargs={"k": 10})

# # T·∫°o LLM t·ª´ OpenAI
# # llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
# llm = ChatOpenAI(model_name="gpt-4-0125-preview", temperature=0.2)

# # T·∫°o c√°c RetrievalQA chains
# malware_tech_chain = RetrievalQA.from_chain_type(llm=llm, retriever=malware_tech_retriever, return_source_documents=True)
# tech_to_malware_chain = RetrievalQA.from_chain_type(llm=llm, retriever=tech_to_malware_retriever, return_source_documents=True)
# # malware_use_chain = RetrievalQA.from_chain_type(llm=llm, retriever=malware_use_retriever, return_source_documents=True)
# # malware_use_pdff = RetrievalQA.from_chain_type(llm=llm, retriever=malware_use_retriever_pdf, return_source_documents=True)
# # malware_use_all_chain = RetrievalQA.from_chain_type(llm=llm, retriever=malware_use_retriever_all, return_source_documents=True)

# # L·∫•y c√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng
# query = input(" Nh·∫≠p c√¢u h·ªèi: ").strip().lower()

# # Ch·ªçn h∆∞·ªõng truy v·∫•n d·ª±a v√†o n·ªôi dung c√¢u h·ªèi
# try:
#     if "m√¥ t·∫£" in query or "h√†nh vi" in query or "use" in query or "beacon" in query:
#         print("\n--------- Truy v·∫•n theo chi·ªÅu: USE (m√¥ t·∫£ h√†nh vi) ‚ûù Malware")
#         # input_key = list(malware_use_chain.input_keys)[0]
#         # rag_result = malware_use_chain.invoke({input_key: query})

#     elif "k·ªπ thu·∫≠t" in query and ("malware" in query or "ph·∫ßn m·ªÅm" in query):
#         print("\n------------- Truy v·∫•n theo chi·ªÅu: K·ªπ thu·∫≠t ‚ûù Malware")
#         input_key = list(tech_to_malware_chain.input_keys)[0]
#         rag_result = tech_to_malware_chain.invoke({input_key: query})

#     else:
#         print("\n------------ Truy v·∫•n theo chi·ªÅu: Malware ‚ûù K·ªπ thu·∫≠t")
#         input_key = list(malware_tech_chain.input_keys)[0]
#         rag_result = malware_tech_chain.invoke({input_key: query})

#     context = rag_result["result"]
#     sources = rag_result.get("source_documents", [])
#     print(context)

#     # N·∫øu kh√¥ng c√≥ th√¥ng tin r√µ r√†ng t·ª´ CTI ‚ûù fallback GPT
#     # if ("kh√¥ng c√≥ th√¥ng tin" in context.lower()) or len(context.strip()) < 30:
#     #     print("\n<=============> CTI n·ªôi b·ªô kh√¥ng ƒë·ªß th√¥ng tin. ƒêang truy v·∫•n GPT to√†n c·ª•c...")
#     #     fallback_response = llm.predict(query)
#     #     print("\n<=============> K·∫øt qu·∫£ t·ª´ GPT:")
#     #     print(fallback_response)
#     # else:
#     #     print("\n<=============> K·∫øt qu·∫£ t·ª´ CTI n·ªôi b·ªô:")
#     #     print(context)

# except Exception as e:
#     print(" L·ªói khi truy v·∫•n:", e)



# vectordb_all = Chroma(persist_directory="vectordb_all", embedding_function=embedding)

# # T·∫°o 2 retrievers c√≥ filter metadata
# retriever_stix = vectordb_all.as_retriever(search_kwargs={"k": 150, "filter": {"source_type": "stix"}})
# retriever_pdf = vectordb_all.as_retriever(search_kwargs={"k": 150, "filter": {"source_type": "pdf"}})

# # T·∫°o LLM t·ª´ OpenAI
# llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.2)

# # T·∫°o RetrievalQA chain ri√™ng cho STIX v√† PDF
# qa_stix = RetrievalQA.from_chain_type(llm=llm, retriever=retriever_stix, return_source_documents=True)
# qa_pdf = RetrievalQA.from_chain_type(llm=llm, retriever=retriever_pdf, return_source_documents=True)

# # L·∫•y c√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng
# query = input("\U0001F9E0 Nh·∫≠p c√¢u h·ªèi: ").strip().lower()

# # Truy v·∫•n STIX tr∆∞·ªõc, fallback PDF n·∫øu c·∫ßn
# try:
#     input_key = list(qa_stix.input_keys)[0]
#     rag_result = qa_stix.invoke({input_key: query})

#     context = rag_result["result"]
#     sources = rag_result.get("source_documents", [])

#     if ("kh√¥ng c√≥ th√¥ng tin" in context.lower()) or len(context.strip()) < 30:
#         print("\n<=============> STIX kh√¥ng ƒë·ªß th√¥ng tin. Fallback sang PDF...")
#         input_key = list(qa_pdf.input_keys)[0]
#         rag_result = qa_pdf.invoke({input_key: query})
#         context = rag_result["result"]

#     print("\n<=============> K·∫øT QU·∫¢:")
#     print(context)

# except Exception as e:
#     print("\n‚ùå L·ªói khi truy v·∫•n:", e)



##################################################################################################################

# # 1) Kh·ªüi t·∫°o embedding + LLM
# llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.2)

# # 2) Load l·∫°i vector DB v·ª´a build (k·ªπ thu·∫≠t + examples)
# mitre_db = Chroma(
#     persist_directory="vectordb_mitre_attack",
#     embedding_function=embedding
# )
# mitre_retriever = mitre_db.as_retriever(search_kwargs={"k": 2})

# # 3) T·∫°o RetrievalQA chain
# mitre_chain = RetrievalQA.from_chain_type(
#     llm=llm,
#     retriever=mitre_retriever,
#     return_source_documents=True
# )

# # 4) H√†m query context cho m·ªôt feature
# def query_context(feat_type: str, feat_val: str):
#     prompt = (
#         f"Feature h√†nh vi: **{feat_type}** = `{feat_val}`\n"
#         "Theo MITRE ATT&CK, h√†nh vi n√†y th∆∞·ªùng li√™n quan ƒë·∫øn k·ªπ thu·∫≠t n√†o? "
#         "Tr·∫£ l·ªùi ·ªü d·∫°ng:\n"
#         "- **[TXXXX ‚Äì Technique Name]**: m√¥ t·∫£ ng·∫Øn\n"
#         "- V√≠ d·ª• malware s·ª≠ d·ª•ng: <li·ªát k√™ n·∫øu c√≥>\n"
#     )
#     input_key = list(mitre_chain.input_keys)[0]
#     rag_out   = mitre_chain.invoke({input_key: prompt})

#     # K·∫øt qu·∫£ text
#     context = rag_out["result"]

#     # L·∫•y metadata c·ªßa c√°c t√†i li·ªáu ngu·ªìn (external_id + name)
#     sources = [
#         f"{d.metadata.get('external_id')} ‚Äì {d.metadata.get('name')}"
#         for d in rag_out["source_documents"]
#     ]
#     return context, sources

# # 5) Th·ª≠ v·ªõi m·ªôt feature kh√°c: regkey_opened
# feat_type  = "regkey_opened"
# feat_value = "HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\OLE"

# context, sources = query_context(feat_type, feat_value)
# print(f"\nüîπ Feature: {feat_type} = {feat_value}")
# print("‚Üí K·ªπ thu·∫≠t + m√¥ t·∫£:\n", context)
# print("‚Üí Ngu·ªìn (external_id ‚Äì name):", sources)

####################################################################################################################


# 1) Load your feature.json
with open("features.json", "r", encoding="utf-8") as f:
    features = json.load(f)

# 2) Initialize embedding, vector DB, retriever, and LLM
embedding = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
vectordb = Chroma(
    persist_directory="vectordb_mitre_attack",
    embedding_function=embedding
)
retriever = vectordb.as_retriever(search_kwargs={"k": 3})
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.2)
rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

# 3) Helper to flatten nested JSON features into (type, value) pairs
def flatten_features(obj, prefix=None):
    if prefix is None:
        prefix = []
    items = []
    if isinstance(obj, dict):
        for k, v in obj.items():
            items.extend(flatten_features(v, prefix + [k]))
    elif isinstance(obj, list):
        feat_type = "_".join(prefix)
        for val in obj:
            if isinstance(val, (dict, list)):
                val_str = json.dumps(val, ensure_ascii=False)
            else:
                val_str = str(val)
            items.append((feat_type, val_str))
    else:
        feat_type = "_".join(prefix)
        items.append((feat_type, str(obj)))
    return items

flat_feats = flatten_features(features)

# 4) Query function for a single feature
def query_context(feat_type: str, feat_val: str):
    prompt = (
        f"Feature h√†nh vi: **{feat_type}** = `{feat_val}`\n"
        "Theo MITRE ATT&CK, h√†nh vi n√†y li√™n quan ƒë·∫øn t·ªëi ƒëa 2 k·ªπ thu·∫≠t ch√≠nh. "
        "Tr·∫£ l·ªùi ·ªü d·∫°ng:\n"
        "- **[TXXXX ‚Äì Technique Name]**: m√¥ t·∫£ ng·∫Øn\n"
        "- V√≠ d·ª• malware s·ª≠ d·ª•ng: <n·∫øu c√≥>\n"
    )
    input_key = list(rag_chain.input_keys)[0]
    output = rag_chain.invoke({input_key: prompt})
    context = output["result"].strip()
    sources = [
        f"{doc.metadata.get('external_id')} ‚Äì {doc.metadata.get('name')}"
        for doc in output["source_documents"]
    ]
    return context, sources

# 5) Iterate all features, query and collect results
results = []
for feat_type, feat_val in flat_feats:
    context, sources = query_context(feat_type, feat_val)
    results.append(f"Feature: {feat_type} = {feat_val}")
    results.append("‚Üí K·ªπ thu·∫≠t + m√¥ t·∫£:\n" + context)
    results.append("‚Üí Ngu·ªìn: " + (", ".join(sources) if sources else "Kh√¥ng c√≥"))
    results.append("\n")

# 6) Write all results to a text file
with open("feature_context_all_results2.txt", "w", encoding="utf-8") as out:
    out.write("\n".join(results))

print("‚úÖ Saved all feature contexts to 'feature_context_all_results2.txt'")

######################################################################################################3

