{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code khảo sát số lượng API và token (sequence length) đối với mô hình multimodal fusion: xLSTM + GCN\n",
    "- API: 500, 1000, 1500\n",
    "- Token: 1000, 1500, 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-03T09:36:11.079Z",
     "iopub.execute_input": "2025-07-03T05:32:01.273210Z",
     "iopub.status.busy": "2025-07-03T05:32:01.272928Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== [API=500] [SEQ_LEN=1000] ===\n",
      "[INFO] Found 1033 samples\n",
      "[Epoch 1] Train Loss=0.2636, Acc=0.8852 | Val F1=0.9441\n",
      "[Epoch 2] Train Loss=0.0607, Acc=0.9820 | Val F1=0.9937\n",
      "[Epoch 3] Train Loss=0.0178, Acc=0.9931 | Val F1=0.9873\n",
      "[Epoch 4] Train Loss=0.0209, Acc=0.9959 | Val F1=0.9873\n",
      "[Epoch 5] Train Loss=0.0105, Acc=0.9986 | Val F1=0.9873\n",
      "[Epoch 6] Train Loss=0.0087, Acc=0.9986 | Val F1=0.9873\n",
      "[Epoch 7] Train Loss=0.0129, Acc=0.9972 | Val F1=0.9937\n",
      "Early stopping.\n",
      ">>> TEST [API=500] [SEQ_LEN=1000] => Loss=0.0574, Acc=0.9871, TPR=0.9747, FPR=0.0000, F1=0.9872\n",
      "\n",
      "=== [API=500] [SEQ_LEN=1500] ===\n",
      "[INFO] Found 1033 samples\n",
      "[Epoch 1] Train Loss=0.2914, Acc=0.8811 | Val F1=0.8951\n",
      "[Epoch 2] Train Loss=0.0877, Acc=0.9668 | Val F1=0.9634\n",
      "[Epoch 3] Train Loss=0.0314, Acc=0.9917 | Val F1=0.9937\n",
      "[Epoch 4] Train Loss=0.0210, Acc=0.9931 | Val F1=0.9873\n",
      "[Epoch 5] Train Loss=0.0164, Acc=0.9917 | Val F1=0.9937\n",
      "[Epoch 6] Train Loss=0.0128, Acc=0.9972 | Val F1=0.9937\n",
      "[Epoch 7] Train Loss=0.0078, Acc=0.9986 | Val F1=0.9937\n",
      "[Epoch 8] Train Loss=0.0052, Acc=0.9986 | Val F1=0.9937\n",
      "Early stopping.\n",
      ">>> TEST [API=500] [SEQ_LEN=1500] => Loss=0.0990, Acc=0.9871, TPR=0.9747, FPR=0.0000, F1=0.9872\n",
      "\n",
      "=== [API=500] [SEQ_LEN=2000] ===\n",
      "[INFO] Found 1033 samples\n",
      "[Epoch 1] Train Loss=0.3489, Acc=0.8382 | Val F1=0.9172\n",
      "[Epoch 2] Train Loss=0.0793, Acc=0.9654 | Val F1=0.9937\n",
      "[Epoch 3] Train Loss=0.0366, Acc=0.9903 | Val F1=0.9873\n",
      "[Epoch 4] Train Loss=0.0181, Acc=0.9959 | Val F1=0.9262\n",
      "[Epoch 5] Train Loss=0.0353, Acc=0.9945 | Val F1=0.9937\n",
      "[Epoch 6] Train Loss=0.0172, Acc=0.9986 | Val F1=0.9937\n",
      "[Epoch 7] Train Loss=0.0125, Acc=0.9986 | Val F1=0.9937\n",
      "Early stopping.\n",
      ">>> TEST [API=500] [SEQ_LEN=2000] => Loss=0.0128, Acc=0.9935, TPR=1.0000, FPR=0.0132, F1=0.9937\n",
      "\n",
      "=== [API=1500] [SEQ_LEN=1000] ===\n",
      "[INFO] Found 1033 samples\n",
      "[Epoch 1] Train Loss=0.2768, Acc=0.9018 | Val F1=0.9634\n",
      "[Epoch 2] Train Loss=0.0851, Acc=0.9668 | Val F1=0.9634\n",
      "[Epoch 3] Train Loss=0.0315, Acc=0.9903 | Val F1=0.6752\n",
      "[Epoch 4] Train Loss=0.0295, Acc=0.9889 | Val F1=0.9937\n",
      "[Epoch 5] Train Loss=0.0199, Acc=0.9917 | Val F1=0.9937\n",
      "[Epoch 6] Train Loss=0.0085, Acc=0.9986 | Val F1=0.9937\n",
      "[Epoch 7] Train Loss=0.0089, Acc=0.9986 | Val F1=0.9937\n",
      "[Epoch 8] Train Loss=0.0108, Acc=0.9986 | Val F1=0.9937\n",
      "[Epoch 9] Train Loss=0.0093, Acc=0.9986 | Val F1=0.9937\n",
      "Early stopping.\n",
      ">>> TEST [API=1500] [SEQ_LEN=1000] => Loss=0.1055, Acc=0.9806, TPR=0.9620, FPR=0.0000, F1=0.9806\n",
      "\n",
      "=== [API=1500] [SEQ_LEN=1500] ===\n",
      "[INFO] Found 1033 samples\n",
      "[Epoch 1] Train Loss=0.2725, Acc=0.9004 | Val F1=0.9693\n",
      "[Epoch 2] Train Loss=0.0898, Acc=0.9668 | Val F1=0.9937\n",
      "[Epoch 3] Train Loss=0.0245, Acc=0.9931 | Val F1=0.9937\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# xLSTM\n",
    "from xlstm import (\n",
    "    xLSTMBlockStack, xLSTMBlockStackConfig,\n",
    "    mLSTMBlockConfig, mLSTMLayerConfig,\n",
    "    sLSTMBlockConfig, sLSTMLayerConfig,\n",
    "    FeedForwardConfig\n",
    ")\n",
    "\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, json_root, pt_root, max_seq_len=1500):\n",
    "        self.max_len = max_seq_len\n",
    "        self.samples = []\n",
    "        self.vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "        idx = 2\n",
    "        mapping = [\n",
    "            (os.path.join(json_root, 'json-atb-benign-507'), os.path.join(pt_root, 'benign'), 0),\n",
    "            (os.path.join(json_root, 'ransom-5xx-new', 'ransomware'), os.path.join(pt_root, 'ransomware'), 1)\n",
    "        ]\n",
    "        for jdir, pdir, label in mapping:\n",
    "            if not os.path.isdir(jdir) or not os.path.isdir(pdir): continue\n",
    "            for fname in os.listdir(jdir):\n",
    "                if not fname.endswith('.json'): continue\n",
    "                sid = os.path.splitext(fname)[0]\n",
    "                jpath = os.path.join(jdir, fname)\n",
    "                ppath = os.path.join(pdir, f\"{sid}.pt\")\n",
    "                if not os.path.isfile(ppath): continue\n",
    "                feat = self._load_json(jpath)\n",
    "                toks = self._extract_tokens(feat)\n",
    "                for t in toks:\n",
    "                    if t not in self.vocab:\n",
    "                        self.vocab[t] = idx; idx += 1\n",
    "                self.samples.append((ppath, toks, label))\n",
    "\n",
    "    def _load_json(self, path):\n",
    "        with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def _extract_tokens(self, feat):\n",
    "        toks = []\n",
    "        for call in feat.get('api_call_sequence', [])[:1000]:\n",
    "            toks.append(f\"api:{call.get('api','')}\")\n",
    "        for ft, vals in feat.get('behavior_summary', {}).items():\n",
    "            for v in vals: toks.append(f\"feature:{ft}:{v}\")\n",
    "        for d in feat.get('dropped_files', []):\n",
    "            toks.append(f\"dropped_file:{d if not isinstance(d,dict) else d.get('filepath','')}\" )\n",
    "        for sig in feat.get('signatures', []): toks.append(f\"signature:{sig.get('name','')}\" )\n",
    "        for p in feat.get('processes', []): toks.append(f\"process:{p.get('name','')}\" )\n",
    "        for proto, ents in feat.get('network', {}).items():\n",
    "            for e in ents:\n",
    "                if isinstance(e, dict):\n",
    "                    dst = e.get('dst') or e.get('dst_ip','')\n",
    "                    port = e.get('dst_port') or e.get('port','')\n",
    "                    toks.append(f\"network:{proto}:{dst}:{port}\")\n",
    "                else:\n",
    "                    toks.append(f\"network:{proto}:{e}\")\n",
    "        return toks\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        ppath, toks, label = self.samples[i]\n",
    "        data = torch.load(ppath, weights_only=False)\n",
    "        idxs = [self.vocab.get(t, self.vocab['<UNK>']) for t in toks]\n",
    "        if len(idxs) < self.max_len:\n",
    "            idxs += [self.vocab['<PAD>']] * (self.max_len - len(idxs))\n",
    "        else:\n",
    "            idxs = idxs[:self.max_len]\n",
    "        seq = torch.tensor(idxs, dtype=torch.long)\n",
    "        return data, seq, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    graphs, seqs, labels = zip(*batch)\n",
    "    return Batch.from_data_list(graphs), torch.stack(seqs), torch.stack(labels)\n",
    "\n",
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, in_feats, hidden=64, drop=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_feats, hidden)\n",
    "        self.bn1 = BatchNorm(hidden)\n",
    "        self.conv2 = GCNConv(hidden, hidden)\n",
    "        self.bn2 = BatchNorm(hidden)\n",
    "        self.drop = drop\n",
    "        self.output_dim = hidden\n",
    "    def forward(self, x, ei, batch):\n",
    "        x = F.relu(self.bn1(self.conv1(x, ei)))\n",
    "        x = F.dropout(x, self.drop, training=self.training)\n",
    "        x = F.relu(self.bn2(self.conv2(x, ei)))\n",
    "        x = F.dropout(x, self.drop, training=self.training)\n",
    "        return global_mean_pool(x, batch)\n",
    "\n",
    "class xLSTMEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed=128, seq_len=1500, blocks=1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed, padding_idx=0)\n",
    "        cfg = xLSTMBlockStackConfig(\n",
    "            mlstm_block=mLSTMBlockConfig(\n",
    "                mlstm=mLSTMLayerConfig(conv1d_kernel_size=4, qkv_proj_blocksize=4, num_heads=4)\n",
    "            ),\n",
    "            slstm_block=sLSTMBlockConfig(\n",
    "                slstm=sLSTMLayerConfig(backend=\"vanilla\", num_heads=4, conv1d_kernel_size=4, bias_init=\"powerlaw_blockdependent\"),\n",
    "                feedforward=FeedForwardConfig(proj_factor=1.3, act_fn=\"gelu\")\n",
    "            ),\n",
    "            context_length=seq_len,\n",
    "            num_blocks=blocks,\n",
    "            embedding_dim=embed,\n",
    "            slstm_at=[0]\n",
    "        )\n",
    "        self.xlstm = xLSTMBlockStack(cfg)\n",
    "        self.output_dim = embed\n",
    "    def forward(self, seq):\n",
    "        out = self.xlstm(self.embed(seq))\n",
    "        return out.mean(dim=1)\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, hiddens=[128,64], drop=0.3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        dims = [in_dim] + hiddens\n",
    "        for i in range(len(hiddens)):\n",
    "            layers.extend([nn.Linear(dims[i], dims[i+1]), nn.ReLU(), nn.Dropout(drop)])\n",
    "        layers.append(nn.Linear(dims[-1], 1))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x).squeeze(1)\n",
    "\n",
    "class MultiModalClassifier(nn.Module):\n",
    "    def __init__(self, graph_enc, seq_enc, fusion_h=128):\n",
    "        super().__init__()\n",
    "        self.graph_enc = graph_enc\n",
    "        self.seq_enc = seq_enc\n",
    "        fusion_dim = graph_enc.output_dim + seq_enc.output_dim\n",
    "        self.classifier = MLPClassifier(fusion_dim, [fusion_h, fusion_h//2])\n",
    "    def forward(self, g, seq):\n",
    "        g_emb = self.graph_enc(g.x, g.edge_index, g.batch)\n",
    "        s_emb = self.seq_enc(seq)\n",
    "        return self.classifier(torch.cat([g_emb, s_emb], dim=1))\n",
    "\n",
    "def train_epoch(model, loader, crit, opt, device):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for g, seq, labs in loader:\n",
    "        g, seq, labs = g.to(device), seq.to(device), labs.to(device)\n",
    "        opt.zero_grad()\n",
    "        logits = model(g, seq)\n",
    "        loss = crit(logits, labs)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item() * labs.size(0)\n",
    "        preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "        correct += (preds == labs).sum().item()\n",
    "        total += labs.size(0)\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "def eval_metrics(model, loader, crit, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for g, seq, labs in loader:\n",
    "            g, seq, labs = g.to(device), seq.to(device), labs.to(device)\n",
    "            logits = model(g, seq)\n",
    "            total_loss += crit(logits, labs).item() * labs.size(0)\n",
    "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labs.cpu().tolist())\n",
    "    if len(set(all_labels)) < 2:\n",
    "        return {'loss': total_loss, 'acc': 0, 'tpr': 0, 'fpr': 0, 'f1': 0}\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
    "    total = tp + tn + fp + fn\n",
    "    return {\n",
    "        'loss': total_loss/total,\n",
    "        'acc': (tp+tn)/total,\n",
    "        'tpr': tp/(tp+fn+1e-9),\n",
    "        'fpr': fp/(fp+tn+1e-9),\n",
    "        'f1': f1_score(all_labels, all_preds)\n",
    "    }\n",
    "\n",
    "def survey_gcn_xlstm():\n",
    "    json_root = \"/kaggle/input\"\n",
    "    api_options = [500, 1000, 1500,]\n",
    "    seq_len_options = [1000, 1500, 2000]\n",
    "    base_pt_root = \"/kaggle/input\"\n",
    "    batch_size = 8\n",
    "    lr = 1e-3\n",
    "    epochs = 20\n",
    "    patience = 5\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    for api_num in api_options:\n",
    "        pt_root = os.path.join(base_pt_root, f\"{api_num}-final/{api_num}\")\n",
    "        for seq_len in seq_len_options:\n",
    "            print(f\"\\n=== [API={api_num}] [SEQ_LEN={seq_len}] ===\")\n",
    "            ds = MultiModalDataset(json_root, pt_root, max_seq_len=seq_len)\n",
    "            print(f\"[INFO] Found {len(ds)} samples\")\n",
    "            if len(ds) < 10:\n",
    "                print(\"[WARNING] Not enough samples. Skipping...\")\n",
    "                continue\n",
    "            labels = [lbl for _,_,lbl in ds.samples]\n",
    "            outer = StratifiedShuffleSplit(n_splits=1, test_size=0.15, random_state=42)\n",
    "            train_val_idx, test_idx = next(outer.split(range(len(ds)), labels))\n",
    "            inner = StratifiedShuffleSplit(n_splits=1, test_size=0.17647, random_state=42)\n",
    "            y_tv = [labels[i] for i in train_val_idx]\n",
    "            tr_idx_rel, val_idx_rel = next(inner.split(train_val_idx, y_tv))\n",
    "            tr_idx = [train_val_idx[i] for i in tr_idx_rel]\n",
    "            val_idx = [train_val_idx[i] for i in val_idx_rel]\n",
    "\n",
    "            tr_loader = DataLoader(Subset(ds, tr_idx), batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "            val_loader = DataLoader(Subset(ds, val_idx), batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "            test_loader = DataLoader(Subset(ds, test_idx), batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "            sample_g, _, _ = ds[0]\n",
    "            in_feats = sample_g.x.size(1)\n",
    "            graph_enc = GCNEncoder(in_feats).to(device)\n",
    "            seq_enc = xLSTMEncoder(len(ds.vocab), seq_len=seq_len).to(device)\n",
    "            model = MultiModalClassifier(graph_enc, seq_enc).to(device)\n",
    "            crit = nn.BCEWithLogitsLoss()\n",
    "            opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            best_f1 = 0; no_improve = 0; best_state = None\n",
    "            for epoch in range(1, epochs+1):\n",
    "                tr_loss, tr_acc = train_epoch(model, tr_loader, crit, opt, device)\n",
    "                val_met = eval_metrics(model, val_loader, crit, device)\n",
    "                print(f\"[Epoch {epoch}] Train Loss={tr_loss:.4f}, Acc={tr_acc:.4f} | Val F1={val_met['f1']:.4f}\")\n",
    "                if val_met['f1'] > best_f1:\n",
    "                    best_f1 = val_met['f1']\n",
    "                    best_state = (model.state_dict(), opt.state_dict())\n",
    "                    no_improve = 0\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "                    if no_improve >= patience:\n",
    "                        print(\"Early stopping.\")\n",
    "                        break\n",
    "\n",
    "            if best_state:\n",
    "                model.load_state_dict(best_state[0])\n",
    "                opt.load_state_dict(best_state[1])\n",
    "            test_met = eval_metrics(model, test_loader, crit, device)\n",
    "            print(f\">>> TEST [API={api_num}] [SEQ_LEN={seq_len}] => \"\n",
    "                  f\"Loss={test_met['loss']:.4f}, Acc={test_met['acc']:.4f}, \"\n",
    "                  f\"TPR={test_met['tpr']:.4f}, FPR={test_met['fpr']:.4f}, F1={test_met['f1']:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    survey_gcn_xlstm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7725836,
     "sourceId": 12260357,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7727182,
     "sourceId": 12262574,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7790409,
     "sourceId": 12356795,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7790410,
     "sourceId": 12356796,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7790411,
     "sourceId": 12356797,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7793099,
     "sourceId": 12360675,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
