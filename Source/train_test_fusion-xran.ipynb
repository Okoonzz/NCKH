{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c26745",
   "metadata": {},
   "source": [
    "**Đề tài:**\n",
    "Bộ khung phát hiện sớm mã độc tống tiền dựa trên phân tích hành vi kết hợp mồi nhử và tạo tăng cường truy xuất tri thức tấn công\n",
    "(A framework of early ransomware detection using behavior analysis with decoy techniques and RAG-based attack knowledge)\n",
    "- Đinh Lê Thành Công - 22520167\n",
    "- Hồ Hoàng Diệp - 22520249"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa264dea",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-03T17:29:09.812957Z",
     "iopub.status.busy": "2025-07-03T17:29:09.812446Z",
     "iopub.status.idle": "2025-07-03T17:30:28.128832Z",
     "shell.execute_reply": "2025-07-03T17:30:28.128104Z"
    },
    "papermill": {
     "duration": 78.320624,
     "end_time": "2025-07-03T17:30:28.130354",
     "exception": false,
     "start_time": "2025-07-03T17:29:09.809730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xlstm\r\n",
      "  Downloading xlstm-2.0.4-py3-none-any.whl.metadata (24 kB)\r\n",
      "Collecting torch-geometric\r\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from xlstm) (2.6.0+cu124)\r\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from xlstm) (0.8.1)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xlstm) (1.26.4)\r\n",
      "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from xlstm) (3.4.0)\r\n",
      "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from xlstm) (2.3.0)\r\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from xlstm) (4.51.3)\r\n",
      "Collecting reportlab (from xlstm)\r\n",
      "  Downloading reportlab-4.4.2-py3-none-any.whl.metadata (1.8 kB)\r\n",
      "Collecting joypy (from xlstm)\r\n",
      "  Downloading joypy-0.2.6-py2.py3-none-any.whl.metadata (812 bytes)\r\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from xlstm) (6.17.1)\r\n",
      "Requirement already satisfied: dacite in /usr/local/lib/python3.11/dist-packages (from xlstm) (1.9.2)\r\n",
      "Collecting ftfy (from xlstm)\r\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\r\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from xlstm) (1.11.1.4)\r\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from xlstm) (0.31.1)\r\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from xlstm) (14.0.0)\r\n",
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from xlstm) (0.21.1)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from xlstm) (4.67.1)\r\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from xlstm) (0.12.2)\r\n",
      "Collecting mlstm_kernels (from xlstm)\r\n",
      "  Downloading mlstm_kernels-2.0.0-py3-none-any.whl.metadata (25 kB)\r\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.18)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (7.0.0)\r\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.0.9)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\r\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->xlstm) (0.2.13)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->xlstm) (3.18.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->xlstm) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->xlstm) (6.0.2)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->xlstm) (4.13.2)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->xlstm) (1.1.0)\r\n",
      "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (1.8.0)\r\n",
      "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (7.34.0)\r\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (8.6.3)\r\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (0.1.7)\r\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (1.6.0)\r\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (24.0.1)\r\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (6.4.2)\r\n",
      "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (5.7.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\r\n",
      "Requirement already satisfied: scipy>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from joypy->xlstm) (1.15.2)\r\n",
      "Requirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from joypy->xlstm) (2.2.3)\r\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from joypy->xlstm) (3.7.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->xlstm) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->xlstm) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->xlstm) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->xlstm) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->xlstm) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->xlstm) (2.4.1)\r\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf->xlstm) (4.9.3)\r\n",
      "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from reportlab->xlstm) (11.1.0)\r\n",
      "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from reportlab->xlstm) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->xlstm) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->xlstm) (2.19.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->xlstm) (3.4.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->xlstm) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->xlstm) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->xlstm) (12.4.127)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->xlstm)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->xlstm)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->xlstm)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->xlstm)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->xlstm)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->xlstm)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->xlstm) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->xlstm) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->xlstm) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->xlstm)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->xlstm) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->xlstm) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->xlstm) (1.3.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->xlstm) (2024.11.6)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->xlstm) (0.5.3)\r\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->xlstm) (75.2.0)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->xlstm) (0.19.2)\r\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->xlstm) (4.4.2)\r\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->xlstm) (0.7.5)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->xlstm) (3.0.50)\r\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->xlstm) (0.2.0)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->xlstm) (4.9.0)\r\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel->xlstm) (5.7.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel->xlstm) (2.9.0.post0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->xlstm) (0.1.2)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->joypy->xlstm) (1.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->joypy->xlstm) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->joypy->xlstm) (4.57.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->joypy->xlstm) (1.4.8)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.20.0->joypy->xlstm) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.20.0->joypy->xlstm) (2025.2)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->xlstm) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->xlstm) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->xlstm) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->xlstm) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->xlstm) (2024.2.0)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->xlstm) (0.8.4)\r\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-client>=6.1.12->ipykernel->xlstm) (4.3.8)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->xlstm) (0.7.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel->xlstm) (1.17.0)\r\n",
      "Downloading xlstm-2.0.4-py3-none-any.whl (91 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.7/91.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading joypy-0.2.6-py2.py3-none-any.whl (8.6 kB)\r\n",
      "Downloading mlstm_kernels-2.0.0-py3-none-any.whl (349 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.0/349.0 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading reportlab-4.4.2-py3-none-any.whl (2.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: reportlab, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, ftfy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, mlstm_kernels, joypy, xlstm, torch-geometric\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\r\n",
      "Successfully installed ftfy-6.3.1 joypy-0.2.6 mlstm_kernels-2.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 reportlab-4.4.2 torch-geometric-2.6.1 xlstm-2.0.4\r\n"
     ]
    }
   ],
   "source": [
    "!pip install xlstm torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82ceedf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T17:30:28.175832Z",
     "iopub.status.busy": "2025-07-03T17:30:28.175567Z",
     "iopub.status.idle": "2025-07-03T22:04:19.172756Z",
     "shell.execute_reply": "2025-07-03T22:04:19.171958Z"
    },
    "papermill": {
     "duration": 16431.021676,
     "end_time": "2025-07-03T22:04:19.174313",
     "exception": false,
     "start_time": "2025-07-03T17:30:28.152637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Building vocab from scratch…\n",
      "[GCN] Ep01 | TrainL 0.3817 A 0.8755 | ValL 0.0952 A 0.9548 F1 0.9576\n",
      "[GCN] Ep02 | TrainL 0.0949 A 0.9710 | ValL 0.0243 A 0.9935 F1 0.9937\n",
      "[GCN] Ep03 | TrainL 0.0987 A 0.9751 | ValL 0.0779 A 0.9871 F1 0.9875\n",
      "[GCN] Ep04 | TrainL 0.0566 A 0.9820 | ValL 0.0407 A 0.9871 F1 0.9875\n",
      "[GCN] Ep05 | TrainL 0.1053 A 0.9710 | ValL 0.0562 A 0.9871 F1 0.9875\n",
      "[GCN] Ep06 | TrainL 0.0864 A 0.9710 | ValL 0.0298 A 0.9935 F1 0.9937\n",
      "[GCN] Ep07 | TrainL 0.0602 A 0.9820 | ValL 0.0425 A 0.9935 F1 0.9937\n",
      "[GCN] Early stop\n",
      "[GCN] TEST → L 0.0479 A 0.9871 TPR 0.9747 FPR 0.0000 F1 0.9872\n",
      "[xLSTM] Ep01 | TrainL 0.3881 A 0.8575 | ValL 0.1666 A 0.8645 F1 0.8467\n",
      "[xLSTM] Ep02 | TrainL 0.0952 A 0.9640 | ValL 0.0835 A 0.9806 F1 0.9806\n",
      "[xLSTM] Ep03 | TrainL 0.0415 A 0.9834 | ValL 0.0251 A 0.9871 F1 0.9873\n",
      "[xLSTM] Ep04 | TrainL 0.0070 A 0.9986 | ValL 0.0303 A 0.9935 F1 0.9937\n",
      "[xLSTM] Ep05 | TrainL 0.0264 A 0.9945 | ValL 0.2596 A 0.9032 F1 0.9133\n",
      "[xLSTM] Ep06 | TrainL 0.0161 A 0.9945 | ValL 0.0237 A 0.9935 F1 0.9936\n",
      "[xLSTM] Ep07 | TrainL 0.0096 A 0.9972 | ValL 0.0088 A 0.9935 F1 0.9936\n",
      "[xLSTM] Ep08 | TrainL 0.0061 A 0.9972 | ValL 0.0294 A 0.9935 F1 0.9937\n",
      "[xLSTM] Ep09 | TrainL 0.0052 A 0.9986 | ValL 0.0581 A 0.9871 F1 0.9872\n",
      "[xLSTM] Early stop\n",
      "[xLSTM] TEST → L 0.3277 A 0.9613 TPR 0.9241 FPR 0.0000 F1 0.9605\n",
      "\n",
      "[GCN+XLSTM] Ep01 | TrainL 0.3150 A 0.8714 | ValL 0.1694 A 0.9484 F1 0.9474\n",
      "[GCN+XLSTM] Ep02 | TrainL 0.0811 A 0.9696 | ValL 0.0329 A 0.9935 F1 0.9937\n",
      "[GCN+XLSTM] Ep03 | TrainL 0.0247 A 0.9903 | ValL 0.0250 A 0.9935 F1 0.9937\n",
      "[GCN+XLSTM] Ep04 | TrainL 0.0156 A 0.9959 | ValL 0.0240 A 0.9935 F1 0.9937\n",
      "[GCN+XLSTM] Ep05 | TrainL 0.0128 A 0.9972 | ValL 0.0463 A 0.9871 F1 0.9873\n",
      "[GCN+XLSTM] Ep06 | TrainL 0.0119 A 0.9986 | ValL 0.0278 A 0.9935 F1 0.9937\n",
      "[GCN+XLSTM] Ep07 | TrainL 0.0116 A 0.9972 | ValL 0.0244 A 0.9871 F1 0.9873\n",
      "[GCN+XLSTM] Early stop\n",
      "[GCN+XLSTM] TEST → L 0.0717 A 0.9806 TPR 0.9620 FPR 0.0000 F1 0.9806\n",
      "[GCN+XLSTM] Saved best to best_gcn_xlstm.pt\n",
      "\n",
      "\n",
      "[GCN+LSTM] Ep01 | TrainL 0.3523 A 0.8811 | ValL 0.1699 A 0.9806 F1 0.9814\n",
      "[GCN+LSTM] Ep02 | TrainL 0.0775 A 0.9723 | ValL 0.0904 A 0.9806 F1 0.9814\n",
      "[GCN+LSTM] Ep03 | TrainL 0.0227 A 0.9959 | ValL 0.0595 A 0.9677 F1 0.9677\n",
      "[GCN+LSTM] Ep04 | TrainL 0.0551 A 0.9848 | ValL 0.2427 A 0.9355 F1 0.9333\n",
      "[GCN+LSTM] Ep05 | TrainL 0.0682 A 0.9848 | ValL 0.0265 A 0.9871 F1 0.9873\n",
      "[GCN+LSTM] Ep06 | TrainL 0.0257 A 0.9903 | ValL 0.0268 A 0.9871 F1 0.9873\n",
      "[GCN+LSTM] Ep07 | TrainL 0.0119 A 0.9959 | ValL 0.0293 A 0.9871 F1 0.9873\n",
      "[GCN+LSTM] Ep08 | TrainL 0.0254 A 0.9917 | ValL 0.0497 A 0.9806 F1 0.9809\n",
      "[GCN+LSTM] Ep09 | TrainL 0.0110 A 0.9959 | ValL 0.0188 A 0.9935 F1 0.9937\n",
      "[GCN+LSTM] Ep10 | TrainL 0.0133 A 0.9959 | ValL 0.0651 A 0.9806 F1 0.9809\n",
      "[GCN+LSTM] Ep11 | TrainL 0.0530 A 0.9834 | ValL 0.0342 A 0.9871 F1 0.9873\n",
      "[GCN+LSTM] Ep12 | TrainL 0.0147 A 0.9972 | ValL 0.0245 A 0.9871 F1 0.9873\n",
      "[GCN+LSTM] Ep13 | TrainL 0.0055 A 0.9986 | ValL 0.0296 A 0.9871 F1 0.9873\n",
      "[GCN+LSTM] Ep14 | TrainL 0.0207 A 0.9889 | ValL 0.0145 A 0.9935 F1 0.9937\n",
      "[GCN+LSTM] Early stop\n",
      "[GCN+LSTM] TEST → L 0.1186 A 0.9871 TPR 1.0000 FPR 0.0263 F1 0.9875\n",
      "\n",
      "[GAT+XLSTM] Ep01 | TrainL 0.2632 A 0.8935 | ValL 0.0654 A 0.9871 F1 0.9873\n",
      "[GAT+XLSTM] Ep02 | TrainL 0.0653 A 0.9834 | ValL 0.0189 A 0.9935 F1 0.9937\n",
      "[GAT+XLSTM] Ep03 | TrainL 0.0357 A 0.9834 | ValL 0.0176 A 0.9935 F1 0.9937\n",
      "[GAT+XLSTM] Ep04 | TrainL 0.0161 A 0.9917 | ValL 0.0213 A 0.9935 F1 0.9937\n",
      "[GAT+XLSTM] Ep05 | TrainL 0.0121 A 0.9972 | ValL 0.0210 A 0.9935 F1 0.9937\n",
      "[GAT+XLSTM] Ep06 | TrainL 0.0091 A 0.9986 | ValL 0.0226 A 0.9935 F1 0.9937\n",
      "[GAT+XLSTM] Ep07 | TrainL 0.0079 A 0.9972 | ValL 0.0142 A 0.9935 F1 0.9937\n",
      "[GAT+XLSTM] Early stop\n",
      "[GAT+XLSTM] TEST → L 0.0018 A 1.0000 TPR 1.0000 FPR 0.0000 F1 1.0000\n",
      "\n",
      "[GAT+LSTM] Ep01 | TrainL 0.3053 A 0.9059 | ValL 1.6291 A 0.6710 F1 0.5321\n",
      "[GAT+LSTM] Ep02 | TrainL 0.0861 A 0.9723 | ValL 0.0715 A 0.9677 F1 0.9682\n",
      "[GAT+LSTM] Ep03 | TrainL 0.0191 A 0.9945 | ValL 0.0195 A 0.9935 F1 0.9937\n",
      "[GAT+LSTM] Ep04 | TrainL 0.0384 A 0.9876 | ValL 0.0138 A 0.9935 F1 0.9937\n",
      "[GAT+LSTM] Ep05 | TrainL 0.0192 A 0.9972 | ValL 0.0160 A 0.9935 F1 0.9937\n",
      "[GAT+LSTM] Ep06 | TrainL 0.0361 A 0.9848 | ValL 0.0378 A 0.9871 F1 0.9875\n",
      "[GAT+LSTM] Ep07 | TrainL 0.0559 A 0.9876 | ValL 0.0340 A 0.9806 F1 0.9811\n",
      "[GAT+LSTM] Ep08 | TrainL 0.0176 A 0.9931 | ValL 0.0119 A 0.9935 F1 0.9937\n",
      "[GAT+LSTM] Early stop\n",
      "[GAT+LSTM] TEST → L 0.0289 A 0.9935 TPR 1.0000 FPR 0.0132 F1 0.9937\n",
      "\n",
      "[SAGE+XLSTM] Ep01 | TrainL 0.2615 A 0.9032 | ValL 0.0513 A 0.9935 F1 0.9937\n",
      "[SAGE+XLSTM] Ep02 | TrainL 0.0542 A 0.9848 | ValL 0.0294 A 0.9935 F1 0.9937\n",
      "[SAGE+XLSTM] Ep03 | TrainL 0.0331 A 0.9889 | ValL 0.0223 A 0.9935 F1 0.9937\n",
      "[SAGE+XLSTM] Ep04 | TrainL 0.0125 A 0.9972 | ValL 0.0299 A 0.9935 F1 0.9937\n",
      "[SAGE+XLSTM] Ep05 | TrainL 0.0111 A 0.9986 | ValL 0.0310 A 0.9935 F1 0.9937\n",
      "[SAGE+XLSTM] Ep06 | TrainL 0.0178 A 0.9972 | ValL 0.0287 A 0.9935 F1 0.9937\n",
      "[SAGE+XLSTM] Early stop\n",
      "[SAGE+XLSTM] TEST → L 0.0023 A 1.0000 TPR 1.0000 FPR 0.0000 F1 1.0000\n",
      "\n",
      "[SAGE+LSTM] Ep01 | TrainL 0.3324 A 0.8797 | ValL 0.1552 A 0.9613 F1 0.9610\n",
      "[SAGE+LSTM] Ep02 | TrainL 0.1609 A 0.9516 | ValL 0.0669 A 0.9871 F1 0.9873\n",
      "[SAGE+LSTM] Ep03 | TrainL 0.0620 A 0.9820 | ValL 0.0258 A 0.9935 F1 0.9937\n",
      "[SAGE+LSTM] Ep04 | TrainL 0.0164 A 0.9972 | ValL 0.1932 A 0.9032 F1 0.9133\n",
      "[SAGE+LSTM] Ep05 | TrainL 0.0348 A 0.9917 | ValL 0.0244 A 0.9935 F1 0.9937\n",
      "[SAGE+LSTM] Ep06 | TrainL 0.0707 A 0.9779 | ValL 0.0609 A 0.9871 F1 0.9873\n",
      "[SAGE+LSTM] Ep07 | TrainL 0.0221 A 0.9959 | ValL 0.0290 A 0.9935 F1 0.9937\n",
      "[SAGE+LSTM] Ep08 | TrainL 0.0259 A 0.9959 | ValL 0.0293 A 0.9935 F1 0.9937\n",
      "[SAGE+LSTM] Early stop\n",
      "[SAGE+LSTM] TEST → L 0.0450 A 0.9935 TPR 1.0000 FPR 0.0132 F1 0.9937\n",
      "\n",
      "[GIN+XLSTM] Ep01 | TrainL 0.3093 A 0.9059 | ValL 0.1895 A 0.9290 F1 0.9308\n",
      "[GIN+XLSTM] Ep02 | TrainL 0.0863 A 0.9668 | ValL 0.1174 A 0.9613 F1 0.9634\n",
      "[GIN+XLSTM] Ep03 | TrainL 0.0448 A 0.9820 | ValL 0.0195 A 0.9935 F1 0.9937\n",
      "[GIN+XLSTM] Ep04 | TrainL 0.0204 A 0.9945 | ValL 0.1155 A 0.9677 F1 0.9693\n",
      "[GIN+XLSTM] Ep05 | TrainL 0.0071 A 0.9986 | ValL 0.0050 A 1.0000 F1 1.0000\n",
      "[GIN+XLSTM] Ep06 | TrainL 0.0161 A 0.9959 | ValL 0.0673 A 0.9613 F1 0.9634\n",
      "[GIN+XLSTM] Ep07 | TrainL 0.0244 A 0.9917 | ValL 0.0908 A 0.9548 F1 0.9536\n",
      "[GIN+XLSTM] Ep08 | TrainL 0.0054 A 0.9986 | ValL 0.0389 A 0.9935 F1 0.9936\n",
      "[GIN+XLSTM] Ep09 | TrainL 0.0004 A 1.0000 | ValL 0.0349 A 0.9935 F1 0.9936\n",
      "[GIN+XLSTM] Ep10 | TrainL 0.0001 A 1.0000 | ValL 0.0356 A 0.9935 F1 0.9936\n",
      "[GIN+XLSTM] Early stop\n",
      "[GIN+XLSTM] TEST → L 0.1161 A 0.9806 TPR 0.9620 FPR 0.0000 F1 0.9806\n",
      "\n",
      "[GIN+LSTM] Ep01 | TrainL 0.4044 A 0.8354 | ValL 0.1710 A 0.9484 F1 0.9506\n",
      "[GIN+LSTM] Ep02 | TrainL 0.1616 A 0.9391 | ValL 0.1304 A 0.8968 F1 0.8873\n",
      "[GIN+LSTM] Ep03 | TrainL 0.1047 A 0.9668 | ValL 0.0904 A 0.9871 F1 0.9875\n",
      "[GIN+LSTM] Ep04 | TrainL 0.0764 A 0.9723 | ValL 0.0418 A 0.9935 F1 0.9937\n",
      "[GIN+LSTM] Ep05 | TrainL 0.0644 A 0.9793 | ValL 0.0290 A 0.9935 F1 0.9937\n",
      "[GIN+LSTM] Ep06 | TrainL 0.0617 A 0.9889 | ValL 2.8706 A 0.5419 F1 0.6900\n",
      "[GIN+LSTM] Ep07 | TrainL 0.0437 A 0.9834 | ValL 0.0437 A 0.9871 F1 0.9875\n",
      "[GIN+LSTM] Ep08 | TrainL 0.0148 A 0.9972 | ValL 0.0582 A 0.9871 F1 0.9875\n",
      "[GIN+LSTM] Ep09 | TrainL 0.0327 A 0.9931 | ValL 0.0900 A 0.9871 F1 0.9875\n",
      "[GIN+LSTM] Early stop\n",
      "[GIN+LSTM] TEST → L 0.0841 A 0.9742 TPR 1.0000 FPR 0.0526 F1 0.9753\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.nn import global_mean_pool, BatchNorm\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, GINConv\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# ───────────────────────────────────────────\n",
    "# xLSTM imports\n",
    "# ───────────────────────────────────────────\n",
    "from xlstm import (\n",
    "    xLSTMBlockStack,\n",
    "    xLSTMBlockStackConfig,\n",
    "    mLSTMBlockConfig,\n",
    "    mLSTMLayerConfig,\n",
    "    sLSTMBlockConfig,\n",
    "    sLSTMLayerConfig,\n",
    "    FeedForwardConfig\n",
    ")\n",
    "\n",
    "# ───────────────────────────────────────────\n",
    "# Dataset\n",
    "# ───────────────────────────────────────────\n",
    "class MultiModalDataset(Dataset):\n",
    "    CACHE_FILE = 'vocab.json'\n",
    "\n",
    "    def __init__(self, json_root, pt_root, max_seq_len=1500):\n",
    "        self.max_len = max_seq_len\n",
    "        self.samples = []\n",
    "\n",
    "        # vocab\n",
    "        #if os.path.isfile(self.CACHE_FILE):\n",
    "        #    with open(self.CACHE_FILE, 'r', encoding='utf-8') as f:\n",
    "        #        self.vocab = json.load(f)\n",
    "        #    idx = max(self.vocab.values()) + 1\n",
    "        #    print(f\"[INFO] Loaded vocab ({len(self.vocab)} tokens)\")\n",
    "        #else:\n",
    "        self.vocab, idx = {'<PAD>': 0, '<UNK>': 1}, 2\n",
    "        print(\"[INFO] Building vocab from scratch…\")\n",
    "\n",
    "        # iterate folders\n",
    "        mapping = [\n",
    "            (os.path.join(json_root, 'json-atb-benign-507'),\n",
    "             os.path.join(pt_root,  'benign'),      0),\n",
    "            (os.path.join(json_root, 'ransom-5xx-new', 'ransomware'),\n",
    "             os.path.join(pt_root,  'ransomware'),  1)\n",
    "        ]\n",
    "        for jdir, pdir, label in mapping:\n",
    "            if not (os.path.isdir(jdir) and os.path.isdir(pdir)):\n",
    "                continue\n",
    "            for fname in os.listdir(jdir):\n",
    "                if not fname.endswith('.json'):\n",
    "                    continue\n",
    "                sid   = os.path.splitext(fname)[0]\n",
    "                jpath = os.path.join(jdir, fname)\n",
    "                ppath = os.path.join(pdir, f\"{sid}.pt\")\n",
    "                if not os.path.isfile(ppath):\n",
    "                    continue\n",
    "\n",
    "                feat = self._load_json(jpath)\n",
    "                toks = self._extract_tokens(feat)\n",
    "\n",
    "                if not os.path.isfile(self.CACHE_FILE):\n",
    "                    for t in toks:\n",
    "                        if t not in self.vocab:\n",
    "                            self.vocab[t] = idx\n",
    "                            idx += 1\n",
    "\n",
    "                self.samples.append((ppath, toks, label))\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_json(path):\n",
    "        with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def _extract_tokens(self, feat):\n",
    "        toks = []\n",
    "        for call in feat.get('api_call_sequence', [])[:1000]:\n",
    "            toks.append(f\"api:{call.get('api','')}\")\n",
    "        for ft, vals in feat.get('behavior_summary', {}).items():\n",
    "            toks += [f\"feature:{ft}:{v}\" for v in vals]\n",
    "        for d in feat.get('dropped_files', []):\n",
    "            toks.append(f\"dropped:{d if not isinstance(d,dict) else d.get('filepath','')}\")\n",
    "        toks += [f\"sig:{s.get('name','')}\" for s in feat.get('signatures', [])]\n",
    "        toks += [f\"proc:{p.get('name','')}\" for p in feat.get('processes', [])]\n",
    "        for proto, ents in feat.get('network', {}).items():\n",
    "            for e in ents:\n",
    "                if isinstance(e, dict):\n",
    "                    dst  = e.get('dst') or e.get('dst_ip','')\n",
    "                    port = e.get('dst_port') or e.get('port','')\n",
    "                    toks.append(f\"net:{proto}:{dst}:{port}\")\n",
    "                else:\n",
    "                    toks.append(f\"net:{proto}:{e}\")\n",
    "        return toks\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        ppath, toks, label = self.samples[i]\n",
    "        graph = torch.load(ppath, weights_only=False)\n",
    "\n",
    "        idxs = [self.vocab.get(t, 1) for t in toks]\n",
    "        idxs = idxs[:self.max_len] + [0] * max(0, self.max_len - len(idxs))\n",
    "        seq  = torch.tensor(idxs, dtype=torch.long)\n",
    "        return graph, seq, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    graphs, seqs, labels = zip(*batch)\n",
    "    return Batch.from_data_list(graphs), torch.stack(seqs), torch.stack(labels)\n",
    "\n",
    "# ───────────────────────────────────────────\n",
    "# Encoders\n",
    "# ───────────────────────────────────────────\n",
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, in_feats, hidden=64, drop=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_feats, hidden)\n",
    "        self.bn1   = BatchNorm(hidden)\n",
    "        self.conv2 = GCNConv(hidden, hidden)\n",
    "        self.bn2   = BatchNorm(hidden)\n",
    "        self.drop  = drop\n",
    "        self.output_dim = hidden\n",
    "\n",
    "    def forward(self, x, ei, batch):\n",
    "        x = F.relu(self.bn1(self.conv1(x, ei)))\n",
    "        x = F.dropout(x, self.drop, training=self.training)\n",
    "        x = F.relu(self.bn2(self.conv2(x, ei)))\n",
    "        x = F.dropout(x, self.drop, training=self.training)\n",
    "        return global_mean_pool(x, batch)\n",
    "\n",
    "class GATEncoder(GCNEncoder):\n",
    "    def __init__(self, in_feats, hidden=64, drop=0.3):\n",
    "        super().__init__(in_feats, hidden, drop)\n",
    "        self.conv1 = GATConv(in_feats, hidden, heads=4, concat=False)\n",
    "        self.conv2 = GATConv(hidden, hidden, heads=4, concat=False)\n",
    "\n",
    "class SageEncoder(GCNEncoder):\n",
    "    def __init__(self, in_feats, hidden=64, drop=0.3):\n",
    "        super().__init__(in_feats, hidden, drop)\n",
    "        self.conv1 = SAGEConv(in_feats, hidden)\n",
    "        self.conv2 = SAGEConv(hidden, hidden)\n",
    "\n",
    "class GINEncoder(nn.Module):\n",
    "    def __init__(self, in_feats, hidden=64, drop=0.3):\n",
    "        super().__init__()\n",
    "        nn1 = nn.Sequential(nn.Linear(in_feats, hidden), nn.ReLU(),\n",
    "                            nn.Linear(hidden, hidden))\n",
    "        nn2 = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "                            nn.Linear(hidden, hidden))\n",
    "        self.g1 = GINConv(nn1)\n",
    "        self.g2 = GINConv(nn2)\n",
    "        self.bn1 = BatchNorm(hidden)\n",
    "        self.bn2 = BatchNorm(hidden)\n",
    "        self.drop = drop\n",
    "        self.output_dim = hidden\n",
    "\n",
    "    def forward(self, x, ei, batch):\n",
    "        x = F.relu(self.bn1(self.g1(x, ei)))\n",
    "        x = F.dropout(x, self.drop, training=self.training)\n",
    "        x = F.relu(self.bn2(self.g2(x, ei)))\n",
    "        x = F.dropout(x, self.drop, training=self.training)\n",
    "        return global_mean_pool(x, batch)\n",
    "\n",
    "class xLSTMEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed=128, seq_len=1500):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed, padding_idx=0)\n",
    "        cfg = xLSTMBlockStackConfig(\n",
    "            mlstm_block=mLSTMBlockConfig(\n",
    "                mlstm=mLSTMLayerConfig(conv1d_kernel_size=4,\n",
    "                                       qkv_proj_blocksize=4, num_heads=4)),\n",
    "            slstm_block=sLSTMBlockConfig(\n",
    "                slstm=sLSTMLayerConfig(backend=\"vanilla\", num_heads=4,\n",
    "                                       conv1d_kernel_size=4,\n",
    "                                       bias_init=\"powerlaw_blockdependent\"),\n",
    "                feedforward=FeedForwardConfig(proj_factor=1.3, act_fn=\"gelu\")\n",
    "            ),\n",
    "            context_length=seq_len,\n",
    "            num_blocks=1,\n",
    "            embedding_dim=embed,\n",
    "            slstm_at=[0]\n",
    "        )\n",
    "        self.xlstm = xLSTMBlockStack(cfg)\n",
    "        self.output_dim = embed\n",
    "\n",
    "    def forward(self, seq):\n",
    "        return self.xlstm(self.embed(seq)).mean(dim=1)\n",
    "\n",
    "class LSTMEncoder(nn.Module):          # Giữ lại để dùng cho các combo khác\n",
    "    def __init__(self, vocab_size, embed=128, hidden=128):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed, padding_idx=0)\n",
    "        self.lstm  = nn.LSTM(embed, hidden, batch_first=True)\n",
    "        self.output_dim = hidden\n",
    "\n",
    "    def forward(self, seq):\n",
    "        _, (hn, _) = self.lstm(self.embed(seq))\n",
    "        return hn.squeeze(0)\n",
    "\n",
    "# ───────────────────────────────────────────\n",
    "# Classifiers & wrappers\n",
    "# ───────────────────────────────────────────\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, hiddens=[128, 64], drop=0.3):\n",
    "        super().__init__()\n",
    "        dims, layers = [in_dim] + hiddens, []\n",
    "        for i in range(len(hiddens)):\n",
    "            layers += [nn.Linear(dims[i], dims[i+1]), nn.ReLU(), nn.Dropout(drop)]\n",
    "        layers.append(nn.Linear(dims[-1], 1))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x): return self.mlp(x).squeeze(1)\n",
    "\n",
    "class GraphOnly(nn.Module):\n",
    "    def __init__(self, enc): super().__init__(); self.enc=enc; self.fc=MLPClassifier(enc.output_dim)\n",
    "    def forward(self, g,s): return self.fc(self.enc(g.x,g.edge_index,g.batch))\n",
    "\n",
    "class SeqOnly(nn.Module):\n",
    "    def __init__(self, enc): super().__init__(); self.enc=enc; self.fc=MLPClassifier(enc.output_dim)\n",
    "    def forward(self, g,s): return self.fc(self.enc(s))\n",
    "\n",
    "class MultiModal(nn.Module):\n",
    "    def __init__(self, genc, senc, hid=128):\n",
    "        super().__init__()\n",
    "        self.genc, self.senc = genc, senc\n",
    "        self.fc = MLPClassifier(genc.output_dim + senc.output_dim,\n",
    "                                [hid, hid//2])\n",
    "    def forward(self, g,s):\n",
    "        return self.fc(torch.cat([self.genc(g.x,g.edge_index,g.batch),\n",
    "                                  self.senc(s)],1))\n",
    "\n",
    "# ───────────────────────────────────────────\n",
    "# Train & eval\n",
    "# ───────────────────────────────────────────\n",
    "def train_epoch(model, loader, crit, opt, dev):\n",
    "    model.train(); tot, ok, n = 0,0,0\n",
    "    for g,s,l in loader:\n",
    "        g,s,l = g.to(dev),s.to(dev),l.to(dev)\n",
    "        opt.zero_grad()\n",
    "        logit = model(g,s); loss = crit(logit,l); loss.backward(); opt.step()\n",
    "        tot += loss.item()*l.size(0)\n",
    "        ok  += ((torch.sigmoid(logit)>.5)==l).sum().item(); n += l.size(0)\n",
    "    return tot/n, ok/n\n",
    "\n",
    "def metrics(model, loader, crit, dev):\n",
    "    model.eval(); tot, p, y = 0,[],[]\n",
    "    with torch.no_grad():\n",
    "        for g,s,l in loader:\n",
    "            g,s,l = g.to(dev),s.to(dev),l.to(dev)\n",
    "            logit = model(g,s); tot += crit(logit,l).item()*l.size(0)\n",
    "            p+= (torch.sigmoid(logit)>.5).float().cpu().tolist(); y+=l.cpu().tolist()\n",
    "    tn,fp,fn,tp = confusion_matrix(y,p).ravel(); n=tp+tn+fp+fn\n",
    "    return {'loss':tot/n,'acc':(tp+tn)/n,'tpr':tp/(tp+fn+1e-9),\n",
    "            'fpr':fp/(fp+tn+1e-9),'f1':f1_score(y,p)}\n",
    "\n",
    "def run(name, model, tl, vl, te, lr, ep, patience, dev, ckpt=None):\n",
    "    crit = nn.BCEWithLogitsLoss(); opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    best, bad=0,0; best_state=None\n",
    "    for i in range(1,ep+1):\n",
    "        tloss,tacc = train_epoch(model,tl,crit,opt,dev)\n",
    "        v = metrics(model,vl,crit,dev)\n",
    "        print(f\"[{name}] Ep{i:02d} | TrainL {tloss:.4f} A {tacc:.4f} \"\n",
    "              f\"| ValL {v['loss']:.4f} A {v['acc']:.4f} F1 {v['f1']:.4f}\")\n",
    "        if v['f1']>best:\n",
    "            best=v['f1']; best_state=model.state_dict(); bad=0\n",
    "            if ckpt: torch.save(best_state, ckpt)\n",
    "        else:\n",
    "            bad+=1\n",
    "            if bad>=patience: print(f\"[{name}] Early stop\"); break\n",
    "    model.load_state_dict(best_state)\n",
    "    t = metrics(model,te,crit,dev)\n",
    "    print(f\"[{name}] TEST → L {t['loss']:.4f} A {t['acc']:.4f} \"\n",
    "          f\"TPR {t['tpr']:.4f} FPR {t['fpr']:.4f} F1 {t['f1']:.4f}\")\n",
    "    if ckpt: print(f\"[{name}] Saved best to {ckpt}\\n\")\n",
    "\n",
    "# ───────────────────────────────────────────\n",
    "# Main\n",
    "# ───────────────────────────────────────────\n",
    "def main():\n",
    "    json_root   = \"/kaggle/input\"\n",
    "    pt_root     = \"/kaggle/input/1000-final/1000\" # Chỉnh API tại đây\n",
    "    max_len     = 2000\n",
    "    bs, lr, ep, patience = 8, 1e-3, 20, 5\n",
    "    dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    ds = MultiModalDataset(json_root, pt_root, max_len)\n",
    "    labels = [l for _,_,l in ds.samples]\n",
    "\n",
    "    outer = StratifiedShuffleSplit(1, test_size=.15, random_state=42)\n",
    "    tv_idx, test_idx = next(outer.split(range(len(ds)), labels))\n",
    "    inner = StratifiedShuffleSplit(1, test_size=.17647, random_state=42)\n",
    "    y_tv = [labels[i] for i in tv_idx]\n",
    "    tr_rel, va_rel = next(inner.split(tv_idx, y_tv))\n",
    "    tr_idx = [tv_idx[i] for i in tr_rel]; va_idx=[tv_idx[i] for i in va_rel]\n",
    "\n",
    "    tl = DataLoader(Subset(ds,tr_idx),batch_size=bs,shuffle=True,\n",
    "                     collate_fn=collate_fn)\n",
    "    vl = DataLoader(Subset(ds,va_idx),batch_size=bs,shuffle=False,\n",
    "                     collate_fn=collate_fn)\n",
    "    te = DataLoader(Subset(ds,test_idx),batch_size=bs,shuffle=False,\n",
    "                     collate_fn=collate_fn)\n",
    "\n",
    "    # GCN-only\n",
    "    gfeat = ds[0][0].x.size(1)\n",
    "    run(\"GCN\",\n",
    "        GraphOnly(GCNEncoder(gfeat).to(dev)).to(dev),\n",
    "        tl,vl,te, lr,ep,patience,dev)\n",
    "\n",
    "    # xLSTM-only\n",
    "    run(\"xLSTM\",\n",
    "        SeqOnly(xLSTMEncoder(len(ds.vocab),seq_len=max_len).to(dev)).to(dev),\n",
    "        tl,vl,te, lr,ep,patience,dev)\n",
    "\n",
    "    # combos\n",
    "    g_encoders = {'gcn': GCNEncoder, 'gat': GATEncoder, 'sage': SageEncoder, 'gin': GINEncoder}\n",
    "    s_encoders = {'xlstm': xLSTMEncoder, 'lstm': LSTMEncoder}\n",
    "\n",
    "    for gn,gc in g_encoders.items():\n",
    "        for sn,sc in s_encoders.items():\n",
    "            print()\n",
    "            genc = gc(gfeat).to(dev)\n",
    "            senc = sc(len(ds.vocab), seq_len=max_len) if sn=='xlstm' \\\n",
    "                   else sc(len(ds.vocab))\n",
    "            senc = senc.to(dev)\n",
    "            ckpt = 'best_gcn_xlstm.pt' if gn=='gcn' and sn=='xlstm' else None\n",
    "            run(f\"{gn.upper()}+{sn.upper()}\",\n",
    "                MultiModal(genc,senc).to(dev),\n",
    "                tl,vl,te, lr,ep,patience,dev, ckpt)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6919246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_xran.py  –  PyTorch ≥2.0\n",
    "import os, json, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- HYPER ----------\n",
    "MAX_API, MAX_DLL, MAX_MUTEX = 500, 10, 10\n",
    "SEQ_LEN = MAX_API + MAX_DLL + MAX_MUTEX      # 520\n",
    "EMB_DIM = 128\n",
    "BATCH, EPOCHS, PATIENCE, LR = 64, 20, 5, 1e-3\n",
    "SEED = 42; torch.manual_seed(SEED)\n",
    "\n",
    "# ---------- DATASET ----------\n",
    "class XRanDataset(Dataset):\n",
    "    def __init__(self, benign_dir, ransom_dir):\n",
    "        self.samples = []\n",
    "        self.vocab, nxt = {'<PAD>':0,'<UNK>':1}, 2\n",
    "        for lbl, root in [(0, benign_dir), (1, ransom_dir)]:\n",
    "            if not os.path.isdir(root):\n",
    "                raise FileNotFoundError(root)\n",
    "            for fn in tqdm(os.listdir(root), desc=f\"Parse {root}\"):\n",
    "                if not fn.endswith('.json'): continue\n",
    "                with open(os.path.join(root,fn), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    feat = json.load(f)\n",
    "                toks = self._tokens(feat)\n",
    "                for t in toks:\n",
    "                    if t not in self.vocab:\n",
    "                        self.vocab[t] = nxt; nxt += 1\n",
    "                self.samples.append((toks, lbl))\n",
    "\n",
    "    def _tokens(self, feat):\n",
    "        apis = [f\"api:{c.get('api','')}\" for c in feat.get('api_call_sequence', [])[:MAX_API]]\n",
    "        beh  = feat.get('behavior_summary', {})\n",
    "        dlls = [f\"dll:{d}\"   for d in beh.get('dll_loaded', [])][:MAX_DLL]\n",
    "        mtx  = [f\"mutex:{m}\" for m in beh.get('mutex',      [])][:MAX_MUTEX]\n",
    "        return apis + dlls + mtx\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, i):\n",
    "        toks, lbl = self.samples[i]\n",
    "        ids = [self.vocab.get(t,1) for t in toks] + [0]*(SEQ_LEN - len(toks))\n",
    "        return torch.tensor(ids), torch.tensor(lbl, dtype=torch.float32)\n",
    "\n",
    "def collate(batch):\n",
    "    seq, lbl = zip(*batch)\n",
    "    return torch.stack(seq), torch.stack(lbl)\n",
    "\n",
    "# ---------- MODEL ----------\n",
    "class XRanCNN(nn.Module):\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab, EMB_DIM, padding_idx=0)\n",
    "        self.c1  = nn.Conv1d(EMB_DIM, 128, 5, padding=2)\n",
    "        self.c2  = nn.Conv1d(128, 64, 3, padding=1)\n",
    "        self.pool= nn.MaxPool1d(2)\n",
    "        self.fc1 = nn.Linear((SEQ_LEN//4)*64, 64)\n",
    "        self.drop= nn.Dropout(0.5)\n",
    "        self.out = nn.Linear(64, 1)\n",
    "    def forward(self, seq):\n",
    "        x = self.emb(seq).transpose(1,2)\n",
    "        x = self.pool(F.relu(self.c1(x)))\n",
    "        x = self.pool(F.relu(self.c2(x)))\n",
    "        x = self.drop(F.relu(self.fc1(x.flatten(1))))\n",
    "        return self.out(x).squeeze(1)\n",
    "\n",
    "# ---------- HELPERS ----------\n",
    "def train_epoch(m, ld, crit, opt, dev):\n",
    "    m.train(); tot, ok, n = 0,0,0\n",
    "    for s,l in ld:\n",
    "        s,l=s.to(dev),l.to(dev)\n",
    "        opt.zero_grad(); y=m(s); loss=crit(y,l); loss.backward(); opt.step()\n",
    "        tot+=loss.item()*l.size(0); ok+=((torch.sigmoid(y)>.5)==l).sum().item(); n+=l.size(0)\n",
    "    return tot/n, ok/n\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(m, ld, crit, dev):\n",
    "    m.eval(); tot, p, y = 0,[],[]\n",
    "    for s,l in ld:\n",
    "        s,l=s.to(dev),l.to(dev)\n",
    "        yhat=m(s); tot+=crit(yhat,l).item()*l.size(0)\n",
    "        p+= (torch.sigmoid(yhat)>.5).float().cpu().tolist()\n",
    "        y+= l.cpu().tolist()\n",
    "    tn,fp,fn,tp = confusion_matrix(y,p).ravel(); n=tp+tn+fp+fn\n",
    "    return {'loss':tot/n, 'acc':(tp+tn)/n,\n",
    "            'tpr':tp/(tp+fn+1e-9), 'fpr':fp/(fp+tn+1e-9),\n",
    "            'f1':f1_score(y,p)}\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "def main():\n",
    "    benign_dir = '/kaggle/input/json-atb-benign-507'\n",
    "    ransom_dir = '/kaggle/input/ransom-5xx-new/ransomware'\n",
    "    dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    ds = XRanDataset(benign_dir, ransom_dir)\n",
    "    labels = [lbl for _,lbl in ds.samples]\n",
    "\n",
    "    outer = StratifiedShuffleSplit(1, test_size=.15, random_state=SEED)\n",
    "    tv_idx, te_idx = next(outer.split(range(len(ds)), labels))\n",
    "    inner = StratifiedShuffleSplit(1, test_size=.17647, random_state=SEED)\n",
    "    y_tv = [labels[i] for i in tv_idx]\n",
    "    tr_rel, va_rel = next(inner.split(tv_idx, y_tv))\n",
    "    tr_idx=[tv_idx[i] for i in tr_rel]; va_idx=[tv_idx[i] for i in va_rel]\n",
    "\n",
    "    tl=DataLoader(Subset(ds,tr_idx),BATCH,True, collate_fn=collate)\n",
    "    vl=DataLoader(Subset(ds,va_idx),BATCH,False,collate_fn=collate)\n",
    "    te=DataLoader(Subset(ds,te_idx),BATCH,False,collate_fn=collate)\n",
    "\n",
    "    model = XRanCNN(len(ds.vocab)).to(dev)\n",
    "    crit  = nn.BCEWithLogitsLoss()\n",
    "    opt   = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    best_acc, bad, best_state = 0,0,None\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        trL,trA = train_epoch(model, tl, crit, opt, dev)\n",
    "        v = eval_epoch(model, vl, crit, dev)\n",
    "        print(f\"Ep{ep:02d} | TrL {trL:.4f} A {trA:.4f} | \"\n",
    "              f\"VaL {v['loss']:.4f} A {v['acc']:.4f}\")\n",
    "        if v['acc'] > best_acc:\n",
    "            best_acc, bad, best_state = v['acc'], 0, model.state_dict()\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= PATIENCE:\n",
    "                print(\"Early stopping.\"); break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    t = eval_epoch(model, te, crit, dev)\n",
    "    print(f\"TEST → Loss {t['loss']:.4f} Acc {t['acc']:.4f} \"\n",
    "          f\"TPR {t['tpr']:.4f} FPR {t['fpr']:.4f} F1 {t['f1']:.4f}\")\n",
    "    torch.save(best_state, 'best_xran_cnn.pt')\n",
    "    print(\"Saved best checkpoint to best_xran_cnn.pt\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7725836,
     "sourceId": 12260357,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7727182,
     "sourceId": 12262574,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7790410,
     "sourceId": 12356796,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16516.327771,
   "end_time": "2025-07-03T22:04:22.106652",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-03T17:29:05.778881",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
